
function [xopt, fopt, exitflag] = fminun(obj, gradobj, x0, stoptol, algoflag)

  % get function and gradient at starting point
  [n,~] = size(x0); % get number of variables
  f = obj(x0); % get the value of the function at x0
  grad = gradobj(x0);
  x = x0;
  %set starting step length
  alpha = 0.0005;

  while (all(abs(grad(:)) > stoptol))

    if (algoflag == 1)     % steepest descent
      s = srchsd(grad);
    end

    if (algoflag == 2)
      % use conjugate gradient method
    end

    if (algoflag == 3)
      % use quasi-Newton method
    end

    % find the proper alpha level
    minVal = inf;
    lastStepVal = -inf;
    alphas = [];
    testStep = 0.005;
    xTest = x;
    incrementer = 1;
    iterTestStep = testStep;
    while (minVal >= lastStepVal)
      iterTestStep = iterTestStep * 2;
      % calculate at guessed testStep
      xTest = xTest + iterTestStep * s;
      fTest = obj(xTest);
      % add it to the stored list
      alphas(incrementer,1) = iterTestStep;
      alphas(incrementer,2) = fTest;
      % increment things for the next loop
      minVal = min(minVal, fTest);
      lastStepVal = fTest;
      incrementer = incrementer + 1;
      alphaOpt = iterTestStep;
    end
    %     alphas

    % take a step
    xnew = x + alphaOpt*s;
    fnew = obj(xnew);
    grad = gradobj(xnew)
    f = fnew;
    x = xnew;

  end
  xopt = xnew;
  fopt = fnew;
  exitflag = 0;
end

% get steepest descent search direction as a column vector
function [s] = srchsd(grad)
  mag = sqrt(grad'*grad);
  s = -grad/mag;
end
